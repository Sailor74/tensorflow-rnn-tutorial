{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow RNN tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving from matlab to python: http://www.justinkiggins.com/blog/moving-from-matlab-to-python/\n",
    "- Use Anaconda as a python distribution.\n",
    "- Use Jupyter notebook (comes with anaconda) for anything you would do with cell mode scripts or the command line in matlab.\n",
    "- Use a good text editor (sublime text 3 or atom) for editing/viewing python functions.\n",
    "\n",
    "---\n",
    "\n",
    "Learning numpy: http://cs231n.github.io/python-numpy-tutorial/\n",
    "\n",
    "---\n",
    "\n",
    "Learning matplotlib: http://www.labri.fr/perso/nrougier/teaching/matplotlib/\n",
    "\n",
    "---\n",
    "\n",
    "Caffe, Torch, Theano, TensorFlow -- which to use: http://cs231n.stanford.edu/slides/winter1516_lecture12.pdf\n",
    "- Use Theano or TensorFlow for RNNs\n",
    "- Theano is better now, but TensorFlow might be better in the future (academic vs industry projects, blah blah)\n",
    "\n",
    "Perhaps consider frameworks that sit on top of Theano or TensorFlow:\n",
    "- Keras\n",
    "- Blocks\n",
    "- so many others...\n",
    "\n",
    "---\n",
    "\n",
    "My current favorite TensorFlow tutorial: http://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Hello world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hello = tf.constant('TensorFlow helps the tensors flow.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session() # What is a session?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print sess.run(hello) # Why sess.run()? \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Linear regression\n",
    "Essentially from http://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "slope = 0.5\n",
    "bias = 2\n",
    "sigma = 0.5\n",
    "\n",
    "# Data\n",
    "n_samples = 1000\n",
    "batch_size = 100\n",
    "\n",
    "# Generate data\n",
    "x_data = 10*np.random.random(n_samples) - 5\n",
    "y_data = slope * x_data + bias + sigma*np.random.randn(n_samples)\n",
    "\n",
    "# Reshape: size (n_samples,) to size (n_samples,1)\n",
    "x_data = np.reshape(x_data, (n_samples,1))\n",
    "y_data = np.reshape(y_data, (n_samples,1))\n",
    "\n",
    "# Define tf placeholders\n",
    "X = tf.placeholder(tf.float32, shape=(batch_size, 1))\n",
    "Y = tf.placeholder(tf.float32, shape=(batch_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "with tf.variable_scope(\"linear-regression\"):    \n",
    "    W = tf.get_variable(\"weights\", (1,1), initializer = tf.random_normal_initializer())\n",
    "    b = tf.get_variable(\"bias\", (1,), initializer = tf.constant_initializer(0.0))\n",
    "    y_hat = tf.matmul(X,W) + b\n",
    "    loss = tf.reduce_sum((Y - y_hat)**2/n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\n",
    "opt_operation = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:    \n",
    "    # Initialize variables\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # Gradient descent for _ steps\n",
    "    for i in range(1000):\n",
    "        # Select random minibatch\n",
    "        indices = np.random.choice(n_samples, batch_size)\n",
    "        x_batch, y_batch = x_data[indices], y_data[indices]\n",
    "        _, loss_val = sess.run([opt_operation, loss], feed_dict={X: x_batch, Y: y_batch})\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print \"Iter:\", \"%04d\" % (i), \\\n",
    "                  \"Loss:\", \"{:.4f}\".format(loss_val)\n",
    "\n",
    "    # Plotting\n",
    "    plt.plot(x_data, y_data,'o')\n",
    "    plt.plot(x_data, x_data * sess.run(W) + sess.run(b))    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal # For constructing fun signals.\n",
    "import seaborn as sns # Pretty plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You do not need `rnn` and `rnn_cell` from `tensorflow.models.rnn` in order to implement RNNs using TensorFlow. These modules are useful abstractions, but you can also implement RNNs 'by hand'. It is worth reading the `rnn.py` and `rnn_cell.py` files (on a Mac, just spotlight search for them; they are on your computer).\n",
    "\n",
    "In short, `rnn.py` essentially takes the \"cell\" specified by `rnn_cell.py` and creates the computational graph by unrolling in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input and output data:\n",
    "\n",
    "Many RNN tutorials start with very difficult or specialized problems where the signals live in exotic spaces. The official introductory RNN tutorial for TensorFlow uses an LSTM for natural language processing. Yikes.\n",
    "\n",
    "For this tutorial, we will work with simple signals, an input $u(t)\\in\\mathbb{R}^m$ and an output $y(t)\\in\\mathbb{R}^p$, usually simple sinusoids, square waves, etc, in discrete time, and for $T$ time steps. As matrices, we have our dataset: $U\\in\\mathbb{R}^{T\\times m}$ and $Y\\in\\mathbb{R}^{T\\times p}$.\n",
    "\n",
    "We will eventually need to split signals into smaller sized-batches of size $T_{\\text{batch}}\\times m$. Thus, a 'training example' will be a subsequence of the full, length-$T$ dataset. Some datasets are naturally split into different 'batches'. Here we will consider one length-$T$ dataset without loss of intuition.\n",
    "\n",
    "vanilla = better for grasping tf and its gotchas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Signal parameters\n",
    "cycle = 100 # cycle length\n",
    "T = 10*cycle # total time\n",
    "t = np.arange(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rand_signal(time_, cycle_, components_):\n",
    "    \"\"\" \n",
    "    Create a size (T,) signal with lognormal distribution of component frequencies.\n",
    "    \"\"\"\n",
    "    freq = 2*np.pi/cycle_*np.random.lognormal(mean=0, size=components_)\n",
    "    phi = np.random.rand(components_)*cycle_\n",
    "    z = np.sin(np.outer(time_, freq) + phi)\n",
    "    z = z.sum(axis=1) # add up the sinusoids \n",
    "    z = (z - z.min()) / (z.max() - z.min()) # normalize\n",
    "    z = 2*z - 1 # set range to [-1,1]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create different signals to play with\n",
    "# Each signal set, z_*, will be size (T,n_sig)\n",
    "n_sig = 10 \n",
    "freq = 2*np.pi/cycle*np.linspace(1,5,n_sig)\n",
    "\n",
    "# ∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿\n",
    "phi = np.random.rand(n_sig)*cycle\n",
    "z_sin = np.sin(np.outer(t,freq) + phi)\n",
    "\n",
    "# ◻◻◻◻◻◻◻◻◻◻◻◻◻◻◻◻◻◻◻◻◻◻◻◻◻◻◻◻◻◻\n",
    "phi = np.random.rand(n_sig)*cycle\n",
    "z_square = signal.square(np.outer(t,freq) + phi)\n",
    "\n",
    "# |/|/|/|/|/|/|/|/|/|/|/|/|/|/|/|/|/|/|/|/|/|\n",
    "phi = np.random.rand(n_sig)*cycle\n",
    "z_saw = signal.sawtooth(np.outer(t,freq) + phi)\n",
    "\n",
    "# ◻∿◻∿◻∿◻∿◻∿◻∿◻∿◻∿◻∿◻∿◻∿◻∿◻∿◻∿◻∿◻∿◻∿◻\n",
    "freq = 2*np.pi/cycle*np.linspace(5,20,n_sig)\n",
    "phi = np.random.rand(n_sig)*cycle\n",
    "z_modsq = signal.square(np.outer(freq,t) + phi.reshape([-1,1]),\n",
    "                        duty=(0.5*np.sin(2*np.pi/cycle* 1 *t)+0.5))\n",
    "z_modsq = z_modsq.T\n",
    "\n",
    "# @.^;{&(*[\\\"@@{£*|'}{('£?}@*+.#~=@*+^%[&_\n",
    "z_psrnd = 2*np.round(np.random.rand(T,n_sig))-1\n",
    "\n",
    "# ∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿\n",
    "z_randsin = np.zeros((T,n_sig))\n",
    "for i in range(n_sig):\n",
    "    z_randsin[:,i] = rand_signal(t, cycle, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot signals\n",
    "f, ((ax1,ax2),(ax3,ax4),(ax5,ax6)) = plt.subplots(3,2,sharex='col',sharey='row', figsize=(18,8),\n",
    "                                        subplot_kw=dict(xlim=(0,2*cycle),\n",
    "                                                        ylim=(-1.2,1.2)))\n",
    "ax1.plot(z_sin[:,[0,5,9]])\n",
    "ax2.plot(z_square[:,[0,9]])\n",
    "ax3.plot(z_saw[:,[0,9]])\n",
    "ax4.plot(z_modsq[:,[5]])\n",
    "ax5.plot(z_psrnd[:,[0]])\n",
    "ax6.plot(z_randsin[:,[0,1,2]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs:\n",
    "Our goal is to fit a function, called a (dynamical) system, $F:U\\rightarrow Y$, where $U\\in\\mathbb{R}^{T\\times m}$ and $Y\\in\\mathbb{R}^{T\\times p}$. Generally, the output may depend on inputs from time steps far away. E.g. we might have $y(t)$ depend somehow on $u(t-10)$. RNNs -- more generally, state-space models -- replace all higher-order dependencies with first-order dependencies via a standard mathematical trick: introduce a new, possibly high-dimensional, latent variable $x$. The rough intuition from linear systems is that if there is a 10th-order dependence in your system, you need $x$ to be at least 10-dimensional. As a bonus, state-space models give a realization of $F$ that is causal (nowhere do variables explicitly depend on the future).\n",
    "\n",
    "The term \"order\" is used in the preceding paragraph because the shift operator $\\sigma x(t) = x(t+1)$ can be thought of as a discrete-time analog of a derivative and $\\sigma^{10}x(t)$ would be a \"10th order\" term in a model.\n",
    "\n",
    "An RNN is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    x(t+1) &= f(Ax(t) + Bu(t) + b) \\\\\n",
    "    y(t) &= Cx(t) + c\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "with $u\\in\\mathbb{R}^m$, $x\\in\\mathbb{R}^n$, $y\\in\\mathbb{R}^p$, and $t=0,\\dots\\,T-1$\n",
    "\n",
    "Here, $u$, $y$, $f$ are all known, and we estimate $A$, $B$, $b$, $C$, $c$. We also estimate $x(0)$, which for deterministic systems (like the one written above) uniquely determines $x(t)$ for all $t$. For stochastic systems we need to be careful about the interaction of the system itself and its state sequence. Proper estimation of $x$ requires filtering. (Kalman) filtering can be used in the model validation step, and for some types of problems will be very important.\n",
    "\n",
    "It is entirely possible to just fit $F$ directly without introducing a latent variable. Look at the NARMAX literature, for example.\n",
    "\n",
    "[maybe write both versions of RNNs]\n",
    "\n",
    "#### Some TensorFlow RNN basics:\n",
    "* A *batch* is a size $T_\\text{batch}\\times n$ signal.\n",
    "* Standard: use the variable name `num_steps` to refer to $T_\\text{batch}$.\n",
    "* Standard: use the variable name `batch_size` to refer to the number of batches that are optimized over in parallel in one optimization step.\n",
    "\n",
    "Suppose we had one long $U\\in\\mathbb{R}^{T\\times m}$ input signal and one $Y\\in\\mathbb{R}^{T\\times p}$ output signal. Let us consider four strategies for fitting the function $F:U\\rightarrow Y$ using RNNs. Suppose `T=100`.\n",
    "\n",
    "#### Batch strategies:\n",
    "\n",
    "1. Set `num_steps=100` and `batch_size=1`.\n",
    "2. Set `num_steps=20` and `batch_size=5` and split the batches evenly.    \n",
    "3. Set, say, `num_steps=10` and `batch_size=5`, and let each batch be randomly chosen (one new batch selection per optimization step).\n",
    "4. Set, say, `num_steps=10` and `batch_size=T-num_steps=90`, and consider every possible batch.\n",
    "\n",
    "Strategy 1 is standard BPTT. Have to estimate x(0)\n",
    "\n",
    "Strategy 2: Batch 1 is `t=0` to `t=19`, batch 2 is `t=20` to `t=39`, etc. At each optimization step we use the same 5 batches. The optimization never tries to fit the transition from `t=19` to `t=20` for example. As a consequence of this strategy, we also esimate the state variable at times `t=0,20,40,..`. These are our initial states, one per batch.\n",
    "    \n",
    "Strategy 3 is SGD. We randomly choose 5 initial time points ranging from `0` to `T-num_steps` to specify our 5 batches. There will likely be some overlapping batches. For one optimization step, we esimate the corresponding initial states. For the next optimization step, we select a different set of 5 batches and estimate their corresponding initial states. After enough steps, we may have estimates for each initial state, thus giving us an estimate of the entire state sequence. (This is a hacky way to estimate the state sequence.) [this bad] -- as a solution do the forward pass stuff\n",
    "\n",
    "Strategy 4 should evoke feelings related to block Hankel matrices and is, loosely speaking, the general strategy for linear system identification. For linear systems, this strategy requires `num_steps>=n`, the (a priori unknown) minimal state dimension.\n",
    "\n",
    "[discuss exploding]\n",
    "[3 is the main one]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RNN dimensions\n",
    "m = 3 # input dimension\n",
    "n = 100 # state dimension\n",
    "p = 3 # output dimension\n",
    "\n",
    "# Time and batch parameters\n",
    "num_steps = 10 \n",
    "batch_size = 50\n",
    "T_new = T - num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some batch-related functions\n",
    "def hankelize(a, num_steps_):\n",
    "    \"\"\"\n",
    "    Convert size (T,n) matrix to size (num_steps, T-num_steps, n) tensor.\n",
    "    Different batches can now be selected by a[:,batch_inds,:].\n",
    "    Hankelize is not the correct term.\n",
    "    \"\"\"\n",
    "    dims = a.shape\n",
    "    T_new = dims[0] - num_steps\n",
    "    b = np.zeros((T_new, num_steps, dims[1]))\n",
    "    for i in range(T_new):\n",
    "        b[i,:,:] = a[i:i+num_steps, :]\n",
    "    b = np.transpose(b, (1,0,2)) # \n",
    "    return b\n",
    "\n",
    "def get_rand_batch(a, batch_size_):\n",
    "    \"\"\"\n",
    "    args: \n",
    "        a: size (num_steps, T_new, n) array\n",
    "        batch_size: how many batches to select\n",
    "    return:\n",
    "        inds: indices for initial states of each batch\n",
    "        a: size (num_steps, batch_size, n) array\n",
    "    \"\"\"\n",
    "    inds = np.random.choice(T_new, batch_size)\n",
    "    for i in range(len(a)):\n",
    "        a[i] = a[i][:,inds,:] \n",
    "    return inds, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose our input and output data\n",
    "u_data = z_sin[:,0:m]\n",
    "y_data = z_modsq[:,5:5+p]\n",
    "\n",
    "u_data_h = hankelize(u_data, num_steps) # size (num_steps, T-num_steps, m)\n",
    "y_data_h = hankelize(y_data, num_steps) # size (num_steps, T-num_steps, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow's basic RNN usage:\n",
    "\n",
    "---\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn\n",
    "\n",
    "...\n",
    "\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(n) # Other options include BasicLSTMCell, GRUCell, ...\n",
    "outputs, state = rnn.rnn(cell, input_sequences, initial_state=x0) # or tf.models.rnn.rnn() w/o import\n",
    "```\n",
    "---\n",
    "\n",
    "\n",
    "* `n` is the state dimension\n",
    "\n",
    "\n",
    "* `input_sequences` is a python *list*. Each entry of this list is a TF tensor of shape `(batch_size,m)` where `m` is the dimension of the input. The length of this list is `num_step`.\n",
    "\n",
    "\n",
    "* `initial_state=` takes an input (here, `x0`) of shape `(batch_size,n)` and is self-explanatory.\n",
    "\n",
    "\n",
    "* `BasicRNNCell`  impelements: `output = new_state = tanh(A * state + input).`\n",
    "\n",
    "\n",
    "* For `BasicRNNCell`, note that `new_state` (that is, $x(t+1)$) is the same as `output`. Thus to get $y(t)$ we wave to implement `output = C * state + c` ourselves. I think LSTM's have different `output` and `state` specifications.\n",
    "\n",
    "\n",
    "* Additionally, we will have to implement `input = B * ext_input + b` where `ext_input` is $u(t)$\n",
    "\n",
    "\n",
    "* It is possible to write code that can handle different `batch_size` values. [See here](https://www.tensorflow.org/versions/r0.8/resources/faq.html#tensor-shapes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`rnn.rnn` does the unrolling in time so you don't have to. Here is a code snippet from `ptb_word_lm.py` -- TF's official RNN tutorial -- showing how you might unroll in time by hand:\n",
    "\n",
    "```python\n",
    "outputs = []\n",
    "state = self._initial_state\n",
    "with tf.variable_scope(\"RNN\"):\n",
    "  for time_step in range(num_steps):\n",
    "    if time_step > 0: tf.get_variable_scope().reuse_variables() # .reuse_variables() is a key step here\n",
    "    (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
    "    outputs.append(cell_output)\n",
    "\n",
    "output = tf.reshape(tf.concat(1, outputs), [-1, size])\n",
    "softmax_w = tf.get_variable(\"softmax_w\", [size, vocab_size])\n",
    "softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "loss = tf.nn.seq2seq.sequence_loss_by_example(\n",
    "    [logits],\n",
    "    [tf.reshape(self._targets, [-1])],\n",
    "    [tf.ones([batch_size * num_steps])])\n",
    "self._cost = cost = tf.reduce_sum(loss) / batch_size\n",
    "self._final_state = state\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # It is sometimes good to reset_default_graph(), especially when working in notebooks\n",
    "\n",
    "# Input Placeholders\n",
    "U = tf.placeholder(\"float\", [num_steps, batch_size, m])\n",
    "Y = tf.placeholder(\"float\", [num_steps, batch_size, p])\n",
    "Inds = tf.placeholder(tf.int32, [batch_size,])\n",
    "\n",
    "# State sequence\n",
    "x = tf.Variable(tf.zeros([T,n]), name=\"state\")\n",
    "\n",
    "# parameters\n",
    "weights = {\n",
    "    'B': tf.Variable(tf.random_normal([m,n]), name=\"B\"),\n",
    "    'C': tf.Variable(tf.random_normal([n,p]), name=\"C\")\n",
    "} # note that we don't have 'A' -- this is because 'A' will be implemented when we call rnn_cell()\n",
    "biases = {\n",
    "    'b': tf.Variable(tf.random_normal([1,n]), name=\"b\"),\n",
    "    'c': tf.Variable(tf.random_normal([1,p]), name=\"c\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# U is size (num_steps, batch_size, m), and rnn.rnn() requires we input a python list of length num_steps.\n",
    "# Each entry of the list is a (batch_size,m) matrix.\n",
    "# The tf.split function will handle this conversion:\n",
    "Ul = [tf.squeeze(u_, [0]) for u_ in tf.split(0, num_steps, U)] # Ul stands for \"U list\"\n",
    "Yl = [tf.squeeze(y_, [0]) for y_ in tf.split(0, num_steps, Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input = B*u + b\n",
    "Ul = [tf.matmul(Ul[i], weights['B']) + biases['b'] for i in range(num_steps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"myRNN\"):\n",
    "    cell = tf.nn.rnn_cell.BasicRNNCell(n)\n",
    "    outputs, state = rnn.rnn(cell, Ul, initial_state=tf.gather(x, Inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat = [tf.matmul(outputs[i], weights['C']) + biases['c'] for i in range(num_steps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model validation / analysis\n",
    "u_full = tf.placeholder(\"float\",[T,m])\n",
    "u_full = [u_ for u_ in tf.split(0, T, u_full)] # fix this?\n",
    "u_full = [tf.matmul(u_full[i], weights['B']) + biases['b'] for i in range(T)]\n",
    "with tf.variable_scope(\"myRNN\", reuse=True):\n",
    "    out_full, stat_full = rnn.rnn(cell, u_full, initial_state=tf.gather(x, tf.ones([1], dtype=tf.int32)))\n",
    "y_hat_full = [tf.matmul(out_full[i], weights['C']) + biases['c'] for i in range(T)]\n",
    "y_hat_full = tf.concat(0,y_hat_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note: tf.reduce_mean is the exact same as np.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean((tf.concat(0,y_hat) - tf.concat(0,Yl))**2) # L_2 minimization\n",
    "opt = tf.train.AdamOptimizer(learning_rate = 0.01)\n",
    "opt_op = opt.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.scalar_summary(\"loss\", cost)\n",
    "merged_summary_op = tf.merge_all_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(1000):\n",
    "    # select random minibatch\n",
    "    inds_batch, [u_batch, y_batch] = get_rand_batch([u_data_h, y_data_h], batch_size)\n",
    "    _, loss_val = sess.run([opt_op, cost], feed_dict={U: u_batch, Y: y_batch, Inds: inds_batch})\n",
    "    if i % 100 == 0:\n",
    "        print \"Iter:\", \"%04d\" % (i), \\\n",
    "              \"Loss:\", \"{:.4f}\".format(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_predicted = sess.run(y_hat_full, feed_dict={u_full: u_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f, ((ax1,ax2),(ax3,ax4),(ax5,ax6)) = plt.subplots(3,2,sharex='col',sharey='row', figsize=(18,8),\n",
    "                                        subplot_kw=dict(xlim=(0,2*cycle),\n",
    "                                                        ylim=(-1.2,1.2)))\n",
    "ax1.plot(z_sin[:,[0,5,9]])\n",
    "ax2.plot(z_square[:,[0,5,9]])\n",
    "ax3.plot(z_saw[:,[0,5,9]])\n",
    "ax4.plot(z_modsq[:,[5]])\n",
    "ax5.plot(z_psrnd[:,[0]])\n",
    "ax6.plot(z_randsin[:,[0,1,2]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # initialize\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    # ? \n",
    "    summary_writer = tf.train.SummaryWriter('/tmp/tensorflow_logs',graph=sess.graph)\n",
    "    # gradient descent for _ steps\n",
    "    for i in range(100):\n",
    "        # select random minibatch\n",
    "        inds_batch, [u_batch, y_batch] = get_rand_batch([u_data_h, y_data_h], batch_size)\n",
    "        _, loss_val = sess.run([opt_op, cost], feed_dict={U: u_batch, Y: y_batch, Inds: inds_batch})\n",
    "        if i % 100 == 0:\n",
    "            print \"Iter:\", \"%04d\" % (i), \\\n",
    "                  \"Loss:\", \"{:.4f}\".format(loss_val)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.subplot()\n",
    "    #inputs\n",
    "    plt.subplot(3,2,1)\n",
    "    plt.plot(u_data,'k')\n",
    "    plt.xlim([0,5*cycle])\n",
    "    plt.ylim([-1.5, 1.5])\n",
    "\n",
    "    plt.subplot(3,2,2)\n",
    "    plt.plot(sess.run(x)[:,0:2])\n",
    "    plt.xlim([0,5*cycle])\n",
    "    plt.ylim([-1.5, 1.5])\n",
    "    \n",
    "    plt.subplot(3,2,4)\n",
    "    \n",
    "    plt.plot(sess.run(y_hat, feed_dict={U: u_batch})[:,],'r')\n",
    "\n",
    "    Y_hat = sess.fun(tf.concat(0,y_hat), feed_dict={U: u_batch} \n",
    "    plt.plot(y_data[:,0],'b',linewidth=2)\n",
    "    plt.xlim([0,5*cycle])\n",
    "    plt.ylim([-1.5, 1.5])\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat_out = sess.run(y_hat, feed_dict={u: u_batch, y: y_batch})\n",
    "y_hat_out = np.stack(y_hat_out, axis=0)\n",
    "y_hat_out = np.reshape(y_hat_out, [num_steps*batch_size, m], order=\"F\")\n",
    "\n",
    "y_in_plot = np.reshape(y_in, [num_steps*batch_size, m], order=\"F\")\n",
    "u_in_plot = np.reshape(u_in, [num_steps*batch_size, m], order=\"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "\n",
    "plt.subplot()\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(u_in_plot[:,0],'k')\n",
    "plt.xlim([0,5*cycle])\n",
    "plt.ylim([-1.5, 1.5])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(u_in_plot[:,-1],'k')\n",
    "plt.xlim([0,5*cycle])\n",
    "plt.ylim([-1.5, 1.5])\n",
    "\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(y_hat_out[:,0],'b')\n",
    "plt.plot(y_in_plot[:,0],'r')\n",
    "plt.xlim([0,5*cycle])\n",
    "plt.ylim([-1.5, 1.5])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "\n",
    "plt.plot(y_hat_out[:,-1],'b',linewidth=2)\n",
    "plt.plot(y_in_plot[:,-1],'r')\n",
    "plt.xlim([0,5*cycle])\n",
    "plt.ylim([-1.5, 1.5])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
